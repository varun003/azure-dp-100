{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if 'car dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['./data2/car_prediction2.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path='car-data/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'car-data/*.csv'))\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name='car dataset',\n",
    "                                description='car data',\n",
    "                                tags = {'format':'CSV'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating scripts for Pipeline \n",
    "\n",
    "import os,shutil\n",
    "\n",
    "pipeline_folder = 'car_pipeline2'\n",
    "os.makedirs(pipeline_folder,exist_ok=True)\n",
    "\n",
    "print(pipeline_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220956ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $pipeline_folder/car_preprocessing.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from azureml.core import Run\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
    "parser.add_argument('--processed-data', type=str, dest='processed_data', default='processed_data', help='Folder for results')\n",
    "args = parser.parse_args()\n",
    "save_folder = args.processed_data\n",
    "\n",
    "#get the experiement run context\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "data = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "\n",
    "# applying label encoding\n",
    "pre = preprocessing.LabelEncoder()\n",
    "data1 = data.apply(pre.fit_transform)\n",
    "\n",
    "# Normalize the numeric columns\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['Selling_Price','Present_Price','Kms_Driven','car_age']\n",
    "data1[num_cols] = scaler.fit_transform(data1[num_cols])\n",
    "\n",
    "# Save the prepped data\n",
    "print(\"Saving Data...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder,'data.csv')\n",
    "data1.to_csv(save_path, index=False, header=True)\n",
    "\n",
    "# End the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $pipeline_folder/car_training.py\n",
    "from azureml.core import Run,Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn import preprocessing,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-folder\", type=str, dest='training_folder', help='training data folder')\n",
    "args = parser.parse_args()\n",
    "training_folder = args.training_folder\n",
    "\n",
    "# get the experiement run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "file_path = os.path.join(training_folder,'data.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# separating features and labels\n",
    "x,y = data[['Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission','car_age']].values,data1['Owner'].values\n",
    "\n",
    "\n",
    "# splitting the dataset into training and testing\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=123)\n",
    "\n",
    "# Train adecision tree model\n",
    "print('Training a decision tree model...')\n",
    "model = DecisionTreeClassifier().fit(x_train,y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(x_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(x_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "print(\"Saving model...\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'car_model3.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'car_model3',\n",
    "               tags={'Training context':'Pipeline'},\n",
    "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"car-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2) #STANDARD_D1\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new enviroment for our model\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "car_pipeline_env = Environment(\"car_pipeline_env\")\n",
    "car_pipeline_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "car_pipeline_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies\n",
    "packages = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','pyarrow'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "car_pipeline_env.python.conda_dependencies = packages\n",
    "\n",
    "# Register the environment \n",
    "car_pipeline_env.register(workspace=ws)\n",
    "car_pipeline_env = Environment.get(ws, 'car_pipeline_env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "print(car_pipeline_env.name, 'defined.')\n",
    "\n",
    "# # Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# # Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = car_pipeline_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad006a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and running Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Get the training dataset\n",
    "car_ds = ws.datasets.get(\"car dataset\")\n",
    "# data=pd.read_csv('car_pipeline2/car_prediction2.csv')\n",
    "\n",
    "# Create a PipelineData (temporary Data Reference) for the model folder\n",
    "processed_data_folder = PipelineData(\"processed_data_folder\", datastore=ws.get_default_datastore())\n",
    "\n",
    "# Step 1, Run the data prep script\n",
    "processing_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = pipeline_folder,\n",
    "                                script_name = \"car_preprocessing.py\",\n",
    "                                arguments = ['--input-data',car_ds.as_named_input('raw_data'),\n",
    "                                             '--processed-data', processed_data_folder],\n",
    "                                outputs=[processed_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "# Step 2, run the training script\n",
    "training_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = pipeline_folder,\n",
    "                                script_name = \"car_training.py\",\n",
    "                                arguments = ['--training-folder', processed_data_folder],\n",
    "                                inputs=[processed_data_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [processing_step, training_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'car-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d71a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fd39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffaf55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
